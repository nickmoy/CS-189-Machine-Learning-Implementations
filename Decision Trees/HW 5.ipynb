{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to install \"gprof2dot\"\n",
    "import io\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import sklearn.model_selection\n",
    "import sklearn.tree\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import genfromtxt\n",
    "from scipy import stats\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from rcviz import callgraph, viz\n",
    "from save_csv import results_to_csv\n",
    "\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-5  # a small number\n",
    "np.random.seed(69420)\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=3, feature_labels=None, m=0):\n",
    "        self.max_depth = max_depth\n",
    "        self.features = feature_labels\n",
    "        self.left, self.right = None, None  # for non-leaf nodes\n",
    "        self.split_idx, self.thresh = None, None  # for non-leaf nodes\n",
    "        self.data, self.pred = None, None  # for leaf nodes\n",
    "        self.m = m\n",
    "\n",
    "    @staticmethod\n",
    "    def information_gain(X, y, thresh):\n",
    "        lsplit = y[X < thresh]\n",
    "        rsplit = y[X >= thresh]\n",
    "        \n",
    "        ans = DecisionTree.gini_impurity(X,y,thresh) - (len(lsplit)*DecisionTree.gini_impurity(X,lsplit,thresh) + len(rsplit)*DecisionTree.gini_impurity(X,rsplit,thresh))/len(y)\n",
    "        return ans\n",
    "\n",
    "    @staticmethod\n",
    "    def gini_impurity(X, y, thresh):\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        \n",
    "        n_yes = 0\n",
    "        n_no = 0\n",
    "        for pred in y:\n",
    "            if pred == 0:\n",
    "                n_no += 1\n",
    "            else:\n",
    "                n_yes += 1\n",
    "        p_yes = n_yes/len(y)\n",
    "        p_no = n_no/len(y)\n",
    "        \n",
    "        return p_yes*(1-p_yes) + p_no*(1-p_no)\n",
    "\n",
    "    def split(self, X, y, idx, thresh):\n",
    "        X0, idx0, X1, idx1 = self.split_test(X, idx=idx, thresh=thresh)\n",
    "        y0, y1 = y[idx0], y[idx1]\n",
    "        return X0, y0, X1, y1\n",
    "\n",
    "    def split_test(self, X, idx, thresh):\n",
    "        idx0 = np.where(X[:, idx] < thresh)[0]\n",
    "        idx1 = np.where(X[:, idx] >= thresh)[0]\n",
    "        X0, X1 = X[idx0, :], X[idx1, :]\n",
    "        return X0, idx0, X1, idx1\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.max_depth > 0 and 0 <= self.m and self.m <= len(X[0]):\n",
    "            # compute entropy gain for all single-dimension splits,\n",
    "            # thresholding with a linear interpolation of 10 values\n",
    "            gains = []\n",
    "            # The following logic prevents thresholding on exactly the minimum\n",
    "            # or maximum values, which may not lead to any meaningful node\n",
    "            # splits.\n",
    "            if self.m == 0:\n",
    "                feature_idxs = np.arange(X.shape[1])\n",
    "            else:\n",
    "                feature_idxs = np.random.choice(np.arange(X.shape[1]), size=m, replace=False)\n",
    "                feature_idxs.sort()\n",
    "                \n",
    "            X_f = X[:, feature_idxs]\n",
    "                \n",
    "            thresh = np.array([\n",
    "                    np.linspace(np.min(X_f[:, i]) + eps, np.max(X_f[:, i]) - eps, num=10)\n",
    "                    for i in range(X_f.shape[1])\n",
    "            ])\n",
    "                \n",
    "            for i in range(X_f.shape[1]):\n",
    "                gains.append([self.information_gain(X_f[:, i], y, t) for t in thresh[i, :]])\n",
    "\n",
    "            gains = np.nan_to_num(np.array(gains))\n",
    "            self.split_idx, thresh_idx = np.unravel_index(np.argmax(gains), gains.shape)\n",
    "            self.thresh = thresh[self.split_idx, thresh_idx]\n",
    "            X0, y0, X1, y1 = self.split(X, y, idx=self.split_idx, thresh=self.thresh)\n",
    "            if X0.size > 0 and X1.size > 0:\n",
    "                self.left = DecisionTree(\n",
    "                    max_depth=self.max_depth - 1, feature_labels=self.features)\n",
    "                self.left.fit(X0, y0)\n",
    "                self.right = DecisionTree(\n",
    "                    max_depth=self.max_depth - 1, feature_labels=self.features)\n",
    "                self.right.fit(X1, y1)\n",
    "            else:\n",
    "                self.max_depth = 0\n",
    "                self.data, self.labels = X, y\n",
    "                self.pred = stats.mode(y).mode[0]\n",
    "        else:\n",
    "            self.data, self.labels = X, y\n",
    "            self.pred = stats.mode(y).mode[0]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.max_depth == 0:\n",
    "            return self.pred * np.ones(X.shape[0])\n",
    "        else:\n",
    "            X0, idx0, X1, idx1 = self.split_test(X, idx=self.split_idx, thresh=self.thresh)\n",
    "            yhat = np.zeros(X.shape[0])\n",
    "            yhat[idx0] = self.left.predict(X0)\n",
    "            yhat[idx1] = self.right.predict(X1)\n",
    "            return yhat\n",
    "\n",
    "\n",
    "class BaggedTrees(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, max_depth=3, params=None, n=200, feature_labels=None, sample_n=0, m=0):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        self.params = params\n",
    "        self.n = n\n",
    "        self.max_depth = max_depth\n",
    "        self.sample_n = sample_n\n",
    "        self.decision_trees = [\n",
    "            DecisionTree(max_depth=self.max_depth, feature_labels=feature_labels, m=m)\n",
    "            for i in range(self.n)\n",
    "        ]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        votes = []\n",
    "        for i in range(len(self.decision_trees)):\n",
    "            if self.sample_n == 0:\n",
    "                rand_idxs = np.random.randint(len(X), size=len(X))\n",
    "            else:\n",
    "                rand_idxs = np.random.randint(len(X), size=self.sample_n)\n",
    "            self.decision_trees[i] = self.decision_trees[i].fit(X[rand_idxs, :], y[rand_idxs])\n",
    "    \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        votes = []\n",
    "        for i in range(len(self.decision_trees)):\n",
    "            votes.append(self.decision_trees[i].predict(X))\n",
    "            \n",
    "        return stats.mode(votes).mode[0]\n",
    "\n",
    "\n",
    "class RandomForest(BaggedTrees):\n",
    "    def __init__(self, max_depth=3, params=None, n=200, m=1, feature_labels=None, sample_n=0):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        BaggedTrees.__init__(self, max_depth=max_depth, params=params, feature_labels=feature_labels, n=n, sample_n=sample_n)\n",
    "        self.m = m\n",
    "\n",
    "\n",
    "class BoostedRandomForest(RandomForest):\n",
    "    def fit(self, X, y):\n",
    "        self.w = np.ones(X.shape[0]) / X.shape[0]  # Weights on data\n",
    "        self.a = np.zeros(self.n)  # Weights on decision trees\n",
    "        # TODO implement function\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # TODO implement function\n",
    "        pass\n",
    "\n",
    "\n",
    "def preprocess(data, fill_mode=True, min_freq=10, onehot_cols=[]):\n",
    "    # fill_mode = False\n",
    "\n",
    "    # Temporarily assign -1 to missing data\n",
    "    data[data == b''] = '-1'\n",
    "\n",
    "    # Hash the columns (used for handling strings)\n",
    "    onehot_encoding = []\n",
    "    onehot_features = []\n",
    "    for col in onehot_cols:\n",
    "        counter = Counter(data[:, col])\n",
    "        for term in counter.most_common():\n",
    "            if term[0] == b'-1':\n",
    "                continue\n",
    "            if term[-1] <= min_freq:\n",
    "                break\n",
    "            onehot_features.append(term[0])\n",
    "            onehot_encoding.append((data[:, col] == term[0]).astype(np.float))\n",
    "        data[:, col] = '0'\n",
    "    onehot_encoding = np.array(onehot_encoding).T\n",
    "    data = np.hstack([np.array(data, dtype=np.float), np.array(onehot_encoding)])\n",
    "\n",
    "    # Replace missing data with the mode value. We use the mode instead of\n",
    "    # the mean or median because this makes more sense for categorical\n",
    "    # features such as gender or cabin type, which are not ordered.\n",
    "    if fill_mode:\n",
    "        for i in range(data.shape[-1]):\n",
    "            mode = stats.mode(data[((data[:, i] < -1 - eps) +\n",
    "                                    (data[:, i] > -1 + eps))][:, i]).mode[0]\n",
    "            data[(data[:, i] > -1 - eps) * (data[:, i] < -1 + eps)][:, i] = mode\n",
    "\n",
    "    return data, onehot_features\n",
    "\n",
    "\n",
    "def evaluate(clf):\n",
    "    print(\"Cross validation\", sklearn.model_selection.cross_val_score(clf, X, y))\n",
    "    if hasattr(clf, \"decision_trees\"):\n",
    "        counter = Counter([t.tree_.feature[0] for t in clf.decision_trees])\n",
    "        first_splits = [(features[term[0]], term[1]) for term in counter.most_common()]\n",
    "        print(\"First splits\", first_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:13: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  del sys.path[0]\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:15: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Part (b): preprocessing the titanic dataset\n",
      "Features: [b'pclass', b'sex', b'age', b'sibsp', b'parch', b'ticket', b'fare', b'cabin', b'embarked', b'male', b'female', b'S', b'C', b'Q']\n",
      "Train/test size: (999, 14) (310, 14)\n",
      "\n",
      "\n",
      "Part 0: constant classifier\n",
      "Accuracy 0.6136136136136137\n",
      "\n",
      "\n",
      "Part (a-b): simplified decision tree\n",
      "Predictions [0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1.\n",
      " 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1.]\n",
      "training accuracy: 0.80125\n",
      "Predictions [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0.\n",
      " 0. 1. 0. 0.]\n",
      "validation accuracy: 0.778894472361809\n",
      "\n",
      "Random Forest:\n",
      "Predictions [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.\n",
      " 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0.\n",
      " 0. 0. 0. 0.]\n",
      "training accuracy: 0.9175\n",
      "Predictions [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "validation accuracy: 0.778894472361809\n",
      "\n",
      "\n",
      "Part (c): sklearn's decision tree\n",
      "Cross validation [0.775      0.775      0.77       0.8        0.75376884]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset = \"titanic\"\n",
    "    params = {\n",
    "        \"max_depth\": 5,\n",
    "        # \"random_state\": 6,\n",
    "        \"min_samples_leaf\": 10,\n",
    "    }\n",
    "    N = 100\n",
    "\n",
    "    if dataset == \"titanic\":\n",
    "        # Load titanic data\n",
    "        path_train = 'data/titanic_training.csv'\n",
    "        data = genfromtxt(path_train, delimiter=',', dtype=None)\n",
    "        path_test = 'data/titanic_testing_data.csv'\n",
    "        test_data = genfromtxt(path_test, delimiter=',', dtype=None)\n",
    "        y = data[1:, 0]  # label = survived\n",
    "        class_names = [\"Died\", \"Survived\"]\n",
    "\n",
    "        labeled_idx = np.where(y != b'')[0]\n",
    "        y = np.array(y[labeled_idx], dtype=np.int)\n",
    "        print(\"\\n\\nPart (b): preprocessing the titanic dataset\")\n",
    "        X, onehot_features = preprocess(data[1:, 1:], onehot_cols=[1, 5, 7, 8])\n",
    "        X = X[labeled_idx, :]\n",
    "        Z, _ = preprocess(test_data[1:, :], onehot_cols=[1, 5, 7, 8])\n",
    "        assert X.shape[1] == Z.shape[1]\n",
    "        features = list(data[0, 1:]) + onehot_features\n",
    "\n",
    "    elif dataset == \"spam\":\n",
    "        features = [\n",
    "            \"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\", \"prescription\", \"creative\",\n",
    "            \"height\", \"featured\", \"differ\", \"width\", \"other\", \"energy\", \"business\", \"message\",\n",
    "            \"volumes\", \"revision\", \"path\", \"meter\", \"memo\", \"planning\", \"pleased\", \"record\", \"out\",\n",
    "            \"semicolon\", \"dollar\", \"sharp\", \"exclamation\", \"parenthesis\", \"square_bracket\",\n",
    "            \"ampersand\"\n",
    "        ]\n",
    "        assert len(features) == 32\n",
    "\n",
    "        # Load spam data\n",
    "        path_train = 'data/spam_data.mat'\n",
    "        data = scipy.io.loadmat(path_train)\n",
    "        X = data['training_data']\n",
    "        y = np.squeeze(data['training_labels'])\n",
    "        Z = data['test_data']\n",
    "        class_names = [\"Ham\", \"Spam\"]\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Dataset %s not handled\" % dataset)\n",
    "\n",
    "    print(\"Features:\", features)\n",
    "    print(\"Train/test size:\", X.shape, Z.shape)\n",
    "\n",
    "    print(\"\\n\\nPart 0: constant classifier\")\n",
    "    print(\"Accuracy\", 1 - np.sum(y) / y.size)\n",
    "\n",
    "    # Training and validation\n",
    "    idx = int(len(X)/5)\n",
    "    val_set = X[:idx]\n",
    "    train_set = X[idx:]\n",
    "    \n",
    "    # Basic decision tree\n",
    "    print(\"\\n\\nPart (a-b): simplified decision tree\")\n",
    "    dt = DecisionTree(max_depth=3, feature_labels=features)\n",
    "    dt.fit(train_set, y[idx:])\n",
    "    y_pred = dt.predict(train_set)\n",
    "    print(\"Predictions\", y_pred[:100])\n",
    "    error = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y[i+idx] != y_pred[i]:\n",
    "            error += 1\n",
    "    print(\"training accuracy:\", 1 - error/len(y_pred))\n",
    "    \n",
    "    y_pred = dt.predict(val_set)\n",
    "    print(\"Predictions\", y_pred[:100])\n",
    "    error = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y[i] != y_pred[i]:\n",
    "            error += 1\n",
    "    print(\"validation accuracy:\", 1 - error/len(y_pred))\n",
    "    \n",
    "    \n",
    "    # Random Forest\n",
    "    print(\"\\nRandom Forest:\")\n",
    "    rf = RandomForest(max_depth=10, feature_labels=features, n=50, m=5)\n",
    "    rf = rf.fit(train_set, y[idx:])\n",
    "        \n",
    "    y_pred = rf.predict(train_set)\n",
    "    error = 0\n",
    "    print(\"Predictions\", y_pred[:100])\n",
    "    for i in range(len(y_pred)):\n",
    "        if y[i+idx] != y_pred[i]:\n",
    "            error += 1\n",
    "    print(\"training accuracy:\", 1 - error/len(y_pred))\n",
    "    \n",
    "    y_pred = rf.predict(val_set)\n",
    "    error = 0\n",
    "    print(\"Predictions\", y_pred[:100])\n",
    "    for i in range(len(y_pred)):\n",
    "        if y[i] != y_pred[i]:\n",
    "            error += 1\n",
    "    print(\"validation accuracy:\", 1 - error/len(y_pred))\n",
    "    \n",
    "    \n",
    "    print(\"\\n\\nPart (c): sklearn's decision tree\")\n",
    "    clf = sklearn.tree.DecisionTreeClassifier(random_state=0, **params)\n",
    "    clf.fit(X, y)\n",
    "    evaluate(clf)\n",
    "    out = io.StringIO()\n",
    "    sklearn.tree.export_graphviz(\n",
    "        clf, out_file=out, feature_names=features, class_names=class_names)\n",
    "    graph = pydot.graph_from_dot_data(out.getvalue())\n",
    "    pydot.graph_from_dot_data(out.getvalue())[0].write_pdf(\"%s-tree.pdf\" % dataset)\n",
    "\n",
    "    # TODO implement and evaluate parts c-h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(Z)\n",
    "results_to_csv(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "depth: 0\n",
      "split_idx: 9\n",
      "thresh: 1e-05\n",
      "\n",
      "depth: 1\n",
      "split_idx: 0\n",
      "thresh: 2.11111\n",
      "split_idx: 0\n",
      "thresh: 1.00001\n",
      "\n",
      "depth: 2\n",
      "split_idx: 2\n",
      "thresh: 58.888883333333325\n",
      "split_idx: 6\n",
      "thresh: 27.816669999999995\n",
      "split_idx: 2\n",
      "thresh: 52.99999666666666\n",
      "split_idx: 2\n",
      "thresh: 32.333334444444446\n",
      "\n",
      "depth: 3\n",
      "split_idx: None\n",
      "thresh: None\n",
      "pred: 1\n",
      "split_idx: None\n",
      "thresh: None\n",
      "pred: 1\n",
      "split_idx: None\n",
      "thresh: None\n",
      "pred: 1\n",
      "split_idx: None\n",
      "thresh: None\n",
      "pred: 0\n",
      "split_idx: None\n",
      "thresh: None\n",
      "pred: 0\n",
      "split_idx: None\n",
      "thresh: None\n",
      "pred: 0\n",
      "split_idx: None\n",
      "thresh: None\n",
      "pred: 0\n",
      "split_idx: None\n",
      "thresh: None\n",
      "pred: 0\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTree(max_depth=3, feature_labels=features)\n",
    "dt = dt.fit(X, y)\n",
    "\n",
    "def level_order(tree):\n",
    "    for d in range(tree.max_depth+1):\n",
    "        print(\"\\ndepth:\", d)\n",
    "        print_level(tree, d)\n",
    "        \n",
    "def print_level(tree, d):\n",
    "    if tree == None:\n",
    "        return\n",
    "    \n",
    "    if d == 0:\n",
    "        if tree.max_depth == 0:\n",
    "            print(\"split_idx:\", tree.split_idx)\n",
    "            print(\"thresh:\", tree.thresh)\n",
    "            print(\"pred:\", tree.pred)\n",
    "        else:\n",
    "            print(\"split_idx:\", tree.split_idx)\n",
    "            print(\"thresh:\", tree.thresh)\n",
    "    else:\n",
    "        print_level(tree.left, d-1)\n",
    "        print_level(tree.right, d-1)\n",
    "        \n",
    "level_order(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=3, feature_labels=None, m=0):\n",
    "        self.max_depth = max_depth\n",
    "        self.features = feature_labels\n",
    "        self.left, self.right = None, None  # for non-leaf nodes\n",
    "        self.split_idx, self.thresh = None, None  # for non-leaf nodes\n",
    "        self.data, self.pred = None, None  # for leaf nodes\n",
    "        self.m = m\n",
    "\n",
    "    @staticmethod\n",
    "    def information_gain(X, y, thresh):\n",
    "        lsplit = y[X < thresh]\n",
    "        rsplit = y[X >= thresh]\n",
    "        \n",
    "        ans = DecisionTree.gini_impurity(X,y,thresh) - (len(lsplit)*DecisionTree.gini_impurity(X,lsplit,thresh) + len(rsplit)*DecisionTree.gini_impurity(X,rsplit,thresh))/len(y)\n",
    "        return ans\n",
    "\n",
    "    @staticmethod\n",
    "    def gini_impurity(X, y, thresh):\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        \n",
    "        n_yes = 0\n",
    "        n_no = 0\n",
    "        for pred in y:\n",
    "            if pred == 0:\n",
    "                n_no += 1\n",
    "            else:\n",
    "                n_yes += 1\n",
    "        p_yes = n_yes/len(y)\n",
    "        p_no = n_no/len(y)\n",
    "        \n",
    "        return p_yes*(1-p_yes) + p_no*(1-p_no)\n",
    "\n",
    "    def split(self, X, y, idx, thresh):\n",
    "        X0, idx0, X1, idx1 = self.split_test(X, idx=idx, thresh=thresh)\n",
    "        y0, y1 = y[idx0], y[idx1]\n",
    "        return X0, y0, X1, y1\n",
    "\n",
    "    def split_test(self, X, idx, thresh):\n",
    "        idx0 = np.where(X[:, idx] < thresh)[0]\n",
    "        idx1 = np.where(X[:, idx] >= thresh)[0]\n",
    "        X0, X1 = X[idx0, :], X[idx1, :]\n",
    "        return X0, idx0, X1, idx1\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.max_depth > 0 and 0 <= self.m and self.m <= len(X[0]):\n",
    "            # compute entropy gain for all single-dimension splits,\n",
    "            # thresholding with a linear interpolation of 10 values\n",
    "            gains = []\n",
    "            # The following logic prevents thresholding on exactly the minimum\n",
    "            # or maximum values, which may not lead to any meaningful node\n",
    "            # splits.\n",
    "            if self.m == 0:\n",
    "                feature_idxs = np.arange(X.shape[1])\n",
    "            else:\n",
    "                feature_idxs = np.random.choice(np.arange(X.shape[1]), size=m, replace=False)\n",
    "                feature_idxs.sort()\n",
    "                \n",
    "            X_f = X[:, feature_idxs]\n",
    "                \n",
    "            thresh = np.array([\n",
    "                    np.linspace(np.min(X_f[:, i]) + eps, np.max(X_f[:, i]) - eps, num=10)\n",
    "                    for i in range(X_f.shape[1])\n",
    "            ])\n",
    "                \n",
    "            for i in range(X_f.shape[1]):\n",
    "                gains.append([self.information_gain(X_f[:, i], y, t) for t in thresh[i, :]])\n",
    "\n",
    "            gains = np.nan_to_num(np.array(gains))\n",
    "            self.split_idx, thresh_idx = np.unravel_index(np.argmax(gains), gains.shape)\n",
    "            self.thresh = thresh[self.split_idx, thresh_idx]\n",
    "            X0, y0, X1, y1 = self.split(X, y, idx=self.split_idx, thresh=self.thresh)\n",
    "            if X0.size > 0 and X1.size > 0:\n",
    "                self.left = DecisionTree(\n",
    "                    max_depth=self.max_depth - 1, feature_labels=self.features)\n",
    "                self.left.fit(X0, y0)\n",
    "                self.right = DecisionTree(\n",
    "                    max_depth=self.max_depth - 1, feature_labels=self.features)\n",
    "                self.right.fit(X1, y1)\n",
    "            else:\n",
    "                self.max_depth = 0\n",
    "                self.data, self.labels = X, y\n",
    "                self.pred = stats.mode(y).mode[0]\n",
    "        else:\n",
    "            self.data, self.labels = X, y\n",
    "            self.pred = stats.mode(y).mode[0]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.max_depth == 0:\n",
    "            return self.pred * np.ones(X.shape[0])\n",
    "        else:\n",
    "            X0, idx0, X1, idx1 = self.split_test(X, idx=self.split_idx, thresh=self.thresh)\n",
    "            yhat = np.zeros(X.shape[0])\n",
    "            yhat[idx0] = self.left.predict(X0)\n",
    "            yhat[idx1] = self.right.predict(X1)\n",
    "            return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['pain', 'private', 'bank', 'money', 'drug', 'spam', 'prescription', 'creative', 'height', 'featured', 'differ', 'width', 'other', 'energy', 'business', 'message', 'volumes', 'revision', 'path', 'meter', 'memo', 'planning', 'pleased', 'record', 'out', 'semicolon', 'dollar', 'sharp', 'exclamation', 'parenthesis', 'square_bracket', 'ampersand']\n",
      "Train/test size: (5172, 32) (5857, 32)\n",
      "\n",
      "\n",
      "Part 0: constant classifier\n",
      "Accuracy 0.7099767981438515\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    params = {\n",
    "        \"max_depth\": 5,\n",
    "        # \"random_state\": 6,\n",
    "        \"min_samples_leaf\": 10,\n",
    "    }\n",
    "    N = 100\n",
    "\n",
    "    dataset == \"spam\"\n",
    "    features = [\n",
    "        \"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\", \"prescription\", \"creative\",\n",
    "        \"height\", \"featured\", \"differ\", \"width\", \"other\", \"energy\", \"business\", \"message\",\n",
    "        \"volumes\", \"revision\", \"path\", \"meter\", \"memo\", \"planning\", \"pleased\", \"record\", \"out\",\n",
    "        \"semicolon\", \"dollar\", \"sharp\", \"exclamation\", \"parenthesis\", \"square_bracket\",\n",
    "        \"ampersand\"\n",
    "    ]\n",
    "    assert len(features) == 32\n",
    "\n",
    "    # Load spam data\n",
    "    path_train = 'data/spam_data.mat'\n",
    "    data = scipy.io.loadmat(path_train)\n",
    "    X = data['training_data']\n",
    "    y = np.squeeze(data['training_labels'])\n",
    "    Z = data['test_data']\n",
    "    class_names = [\"Ham\", \"Spam\"]\n",
    "\n",
    "    print(\"Features:\", features)\n",
    "    print(\"Train/test size:\", X.shape, Z.shape)\n",
    "\n",
    "    print(\"\\n\\nPart 0: constant classifier\")\n",
    "    print(\"Accuracy\", 1 - np.sum(y) / y.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_sample = np.concatenate((X, np.reshape(y, (y.shape[0], 1))), axis=1)\n",
    "np.random.shuffle(spam_sample)\n",
    "\n",
    "idx = int(len(spam_sample)/5)\n",
    "spam_train = spam_sample[idx:, :-1]\n",
    "spam_train_labels = spam_sample[idx:, -1:]\n",
    "spam_val = spam_sample[:idx, :-1]\n",
    "spam_val_labels = spam_sample[:idx, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Part (a-b): simplified decision tree\n",
      "Predictions [0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0.]\n",
      "training accuracy: 0.7955534074432093\n",
      "Predictions [0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 1.]\n",
      "validation accuracy: 0.7969052224371374\n"
     ]
    }
   ],
   "source": [
    "# Basic decision tree\n",
    "print(\"\\n\\nPart (a-b): simplified decision tree\")\n",
    "dt = DecisionTree(max_depth=3, feature_labels=features)\n",
    "dt.fit(spam_train, spam_train_labels)\n",
    "y_pred = dt.predict(spam_train)\n",
    "print(\"Predictions\", y_pred[:100])\n",
    "error = 0\n",
    "for i in range(len(y_pred)):\n",
    "    if spam_train_labels[i] != y_pred[i]:\n",
    "        error += 1\n",
    "print(\"training accuracy:\", 1 - error/len(y_pred))\n",
    "    \n",
    "y_pred = dt.predict(spam_val)\n",
    "print(\"Predictions\", y_pred[:100])\n",
    "error = 0\n",
    "for i in range(len(y_pred)):\n",
    "    if spam_val_labels[i] != y_pred[i]:\n",
    "        error += 1\n",
    "print(\"validation accuracy:\", 1 - error/len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest:\n",
      "Predictions [0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0.]\n",
      "training accuracy: 0.8112614789753504\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "print(\"\\nRandom Forest:\")\n",
    "rf = RandomForest(max_depth=5, feature_labels=features, n=100, m=5, sample_n=200)\n",
    "rf = rf.fit(spam_train, spam_train_labels)\n",
    "        \n",
    "y_pred = rf.predict(spam_train)\n",
    "error = 0\n",
    "print(\"Predictions\", y_pred[:100])\n",
    "for i in range(len(y_pred)):\n",
    "    if spam_train_labels[i] != y_pred[i]:\n",
    "        error += 1\n",
    "print(\"training accuracy:\", 1 - error/len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions [0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 1.]\n",
      "validation accuracy: 0.8075435203094777\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf.predict(spam_val)\n",
    "error = 0\n",
    "print(\"Predictions\", y_pred[:100])\n",
    "for i in range(len(y_pred)):\n",
    "    if spam_val_labels[i] != y_pred[i]:\n",
    "        error += 1\n",
    "print(\"validation accuracy:\", 1 - error/len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Part (a-b): simplified decision tree\n",
      "depth: 1\n",
      "training accuracy: 0.7619623006283229\n",
      "validation accuracy: 0.7678916827852998\n",
      "depth: 5\n",
      "training accuracy: 0.8105364910584824\n",
      "validation accuracy: 0.8104448742746615\n",
      "depth: 10\n",
      "training accuracy: 0.8392943450942485\n",
      "validation accuracy: 0.8239845261121856\n",
      "depth: 15\n",
      "training accuracy: 0.8639439342677622\n",
      "validation accuracy: 0.8220502901353965\n",
      "depth: 20\n",
      "training accuracy: 0.8774770420492992\n",
      "validation accuracy: 0.8220502901353965\n",
      "depth: 25\n",
      "training accuracy: 0.8837602706621557\n",
      "validation accuracy: 0.8201160541586073\n",
      "depth: 30\n",
      "training accuracy: 0.8871435476075399\n",
      "validation accuracy: 0.8220502901353965\n",
      "depth: 35\n",
      "training accuracy: 0.889801836636056\n",
      "validation accuracy: 0.8220502901353965\n",
      "depth: 40\n",
      "training accuracy: 0.889801836636056\n",
      "validation accuracy: 0.8220502901353965\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Depths\n",
    "print(\"\\n\\nPart (a-b): simplified decision tree\")\n",
    "\n",
    "depths = [1,5,10,15,20,25,30,35,40]\n",
    "train_errors = []\n",
    "val_errors = []\n",
    "for depth in depths:\n",
    "    print(\"depth:\", depth)\n",
    "    dt = DecisionTree(max_depth=depth, feature_labels=features)\n",
    "    dt.fit(spam_train, spam_train_labels)\n",
    "\n",
    "    y_pred = dt.predict(spam_train)\n",
    "    error = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if spam_train_labels[i] != y_pred[i]:\n",
    "            error += 1\n",
    "    print(\"training accuracy:\", 1 - error/len(y_pred))\n",
    "    train_errors.append(error)\n",
    "    \n",
    "    y_pred = dt.predict(spam_val)\n",
    "    error = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if spam_val_labels[i] != y_pred[i]:\n",
    "            error += 1\n",
    "    print(\"validation accuracy:\", 1 - error/len(y_pred))\n",
    "    val_errors.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x11d96c150>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(len(train_errors)):\n",
    "    train_errors[i] = 1 - train_errors[i]/len(y_pred)\n",
    "    val_errors[i] = 1 - val_errors[i]/len(y_pred)\n",
    "\n",
    "plt.plot(depths, train_errors, label=\"training\")\n",
    "plt.plot(depths, val_errors, label=\"validation\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-5  # a small number\n",
    "np.random.seed(69420)\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=3, feature_labels=None, m=0):\n",
    "        self.max_depth = max_depth\n",
    "        self.features = feature_labels\n",
    "        self.left, self.right = None, None  # for non-leaf nodes\n",
    "        self.split_idx, self.thresh = None, None  # for non-leaf nodes\n",
    "        self.data, self.pred = None, None  # for leaf nodes\n",
    "        self.m = m\n",
    "\n",
    "    @staticmethod\n",
    "    def information_gain(X, y, thresh):\n",
    "        lsplit = y[X < thresh]\n",
    "        rsplit = y[X >= thresh]\n",
    "        \n",
    "        ans = DecisionTree.gini_impurity(X,y,thresh) - (len(lsplit)*DecisionTree.gini_impurity(X,lsplit,thresh) + len(rsplit)*DecisionTree.gini_impurity(X,rsplit,thresh))/len(y)\n",
    "        return ans\n",
    "\n",
    "    @staticmethod\n",
    "    def gini_impurity(X, y, thresh):\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        \n",
    "        n_yes = 0\n",
    "        n_no = 0\n",
    "        for pred in y:\n",
    "            if pred == 0:\n",
    "                n_no += 1\n",
    "            else:\n",
    "                n_yes += 1\n",
    "        p_yes = n_yes/len(y)\n",
    "        p_no = n_no/len(y)\n",
    "        \n",
    "        return p_yes*(1-p_yes) + p_no*(1-p_no)\n",
    "\n",
    "    def split(self, X, y, idx, thresh):\n",
    "        X0, idx0, X1, idx1 = self.split_test(X, idx=idx, thresh=thresh)\n",
    "        y0, y1 = y[idx0], y[idx1]\n",
    "        return X0, y0, X1, y1\n",
    "\n",
    "    def split_test(self, X, idx, thresh):\n",
    "        idx0 = np.where(X[:, idx] < thresh)[0]\n",
    "        idx1 = np.where(X[:, idx] >= thresh)[0]\n",
    "        X0, X1 = X[idx0, :], X[idx1, :]\n",
    "        return X0, idx0, X1, idx1\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.max_depth > 0 and 0 <= self.m and self.m <= len(X[0]):\n",
    "            # compute entropy gain for all single-dimension splits,\n",
    "            # thresholding with a linear interpolation of 10 values\n",
    "            gains = []\n",
    "            # The following logic prevents thresholding on exactly the minimum\n",
    "            # or maximum values, which may not lead to any meaningful node\n",
    "            # splits.\n",
    "            if self.m == 0:\n",
    "                feature_idxs = np.arange(X.shape[1])\n",
    "            else:\n",
    "                feature_idxs = np.random.choice(np.arange(X.shape[1]), size=m, replace=False)\n",
    "                feature_idxs.sort()\n",
    "                \n",
    "            X_f = X[:, feature_idxs]\n",
    "                \n",
    "            thresh = np.array([\n",
    "                    np.linspace(np.min(X_f[:, i]) + eps, np.max(X_f[:, i]) - eps, num=10)\n",
    "                    for i in range(X_f.shape[1])\n",
    "            ])\n",
    "                \n",
    "            for i in range(X_f.shape[1]):\n",
    "                gains.append([self.information_gain(X_f[:, i], y, t) for t in thresh[i, :]])\n",
    "\n",
    "            gains = np.nan_to_num(np.array(gains))\n",
    "            self.split_idx, thresh_idx = np.unravel_index(np.argmax(gains), gains.shape)\n",
    "            self.thresh = thresh[self.split_idx, thresh_idx]\n",
    "            X0, y0, X1, y1 = self.split(X, y, idx=self.split_idx, thresh=self.thresh)\n",
    "            if X0.size > 0 and X1.size > 0:\n",
    "                self.left = DecisionTree(\n",
    "                    max_depth=self.max_depth - 1, feature_labels=self.features)\n",
    "                self.left.fit(X0, y0)\n",
    "                self.right = DecisionTree(\n",
    "                    max_depth=self.max_depth - 1, feature_labels=self.features)\n",
    "                self.right.fit(X1, y1)\n",
    "            else:\n",
    "                self.max_depth = 0\n",
    "                self.data, self.labels = X, y\n",
    "                self.pred = stats.mode(y).mode[0]\n",
    "        else:\n",
    "            self.data, self.labels = X, y\n",
    "            self.pred = stats.mode(y).mode[0]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.max_depth == 0:\n",
    "            print(self.pred)\n",
    "            return self.pred * np.ones(X.shape[0])\n",
    "        else:\n",
    "            X0, idx0, X1, idx1 = self.split_test(X, idx=self.split_idx, thresh=self.thresh)\n",
    "            if len(X) != 0:\n",
    "                print(\"feat_idx:\", self.split_idx)\n",
    "                print(\"thresh:\", self.thresh)\n",
    "                print(\"\")\n",
    "            yhat = np.zeros(X.shape[0])\n",
    "            yhat[idx0] = self.left.predict(X0)\n",
    "            yhat[idx1] = self.right.predict(X1)\n",
    "            return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Part (a-b): simplified decision tree\n",
      "feat_idx: 28\n",
      "thresh: 1e-05\n",
      "idx1: [0]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "feat_idx: 31\n",
      "thresh: 1e-05\n",
      "idx0: [0]\n",
      "feat_idx: 19\n",
      "thresh: 1e-05\n",
      "idx0: [0]\n",
      "[1.]\n",
      "[0.]\n",
      "[0.]\n",
      "[1.]\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "# Basic decision tree\n",
    "print(\"\\n\\nPart (a-b): simplified decision tree\")\n",
    "dt = DecisionTree(max_depth=3, feature_labels=features)\n",
    "dt.fit(spam_train, spam_train_labels)\n",
    "\n",
    "y_pred = dt.predict(X[0:1,])\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_idx: 28\n",
      "thresh: 1e-05\n",
      "idx0: [   0    2    3 ... 5851 5852 5854]\n",
      "idx1: [   1    5   11 ... 5853 5855 5856]\n",
      "feat_idx: 19\n",
      "thresh: 1e-05\n",
      "idx0: [   0    1    2 ... 3992 3993 3994]\n",
      "idx1: [ 256  477 1772]\n",
      "feat_idx: 29\n",
      "thresh: 1e-05\n",
      "idx0: [   0    1    2 ... 3985 3988 3991]\n",
      "idx1: [   3    4    5 ... 3987 3989 3990]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "feat_idx: 31\n",
      "thresh: 1e-05\n",
      "idx0: [   0    1    3 ... 1858 1859 1861]\n",
      "idx1: [   2   29   33   36   37   42   43   53   60   65   75   78   82   91\n",
      "   93  103  106  113  115  131  145  146  154  170  174  182  188  190\n",
      "  197  203  213  214  215  231  233  240  242  244  247  273  276  280\n",
      "  286  291  293  315  316  319  324  327  340  345  353  356  378  381\n",
      "  386  388  405  408  417  426  432  433  451  459  460  463  469  481\n",
      "  498  499  504  513  514  525  526  554  556  557  559  566  576  578\n",
      "  579  581  590  595  596  614  616  624  640  654  662  676  679  685\n",
      "  691  696  703  713  714  717  727  733  757  759  780  783  786  797\n",
      "  799  800  806  808  810  824  827  829  837  840  848  849  850  855\n",
      "  860  864  867  873  880  884  887  889  892  907  915  918  924  970\n",
      "  973  978  989 1008 1012 1014 1028 1036 1042 1046 1050 1060 1064 1073\n",
      " 1080 1094 1095 1102 1103 1122 1124 1131 1132 1133 1136 1138 1153 1211\n",
      " 1220 1237 1239 1248 1256 1304 1307 1309 1318 1336 1341 1345 1352 1353\n",
      " 1371 1381 1385 1388 1392 1396 1401 1463 1482 1508 1509 1512 1519 1520\n",
      " 1522 1525 1532 1538 1541 1545 1548 1551 1555 1559 1565 1567 1568 1583\n",
      " 1588 1596 1597 1635 1644 1646 1647 1653 1656 1659 1661 1662 1669 1688\n",
      " 1692 1698 1708 1719 1735 1741 1743 1759 1764 1766 1767 1768 1776 1792\n",
      " 1797 1802 1808 1809 1814 1827 1831 1847 1852 1860]\n",
      "feat_idx: 19\n",
      "thresh: 1e-05\n",
      "idx0: [   0    1    2 ... 1611 1612 1613]\n",
      "idx1: [1321]\n",
      "[1.]\n",
      "[0.]\n",
      "feat_idx: 7\n",
      "thresh: 1e-05\n",
      "idx0: [  0   1   2   3   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  47  48  49  50  51  52  53  54  55\n",
      "  56  57  58  59  60  62  63  64  65  66  67  68  69  70  71  72  73  74\n",
      "  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92\n",
      "  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110\n",
      " 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128\n",
      " 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146\n",
      " 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164\n",
      " 165 166 167 168 169 170 171 172 173 174 175 177 178 179 180 181 182 183\n",
      " 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201\n",
      " 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219\n",
      " 220 221 222 223 224 225 226 227 228 229 230 231 233 234 235 236 237 238\n",
      " 239 240 241 242 243 244 245 246 247]\n",
      "idx1: [  4  46  61 176 232]\n",
      "[0.]\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "# Spam Decision Tree submission\n",
    "y_pred = dt.predict(Z)\n",
    "results_to_csv(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
